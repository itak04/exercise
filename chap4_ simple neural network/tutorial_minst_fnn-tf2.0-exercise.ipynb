{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:44:51.218550Z",
     "start_time": "2025-03-19T07:44:46.268915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Program_Files\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:44:54.682999Z",
     "start_time": "2025-03-19T07:44:54.576625Z"
    }
   },
   "outputs": [],
   "source": [
    "class myModel:\n",
    "        def __init__(self):\n",
    "                ####################\n",
    "                '''声明模型对应的参数'''\n",
    "                # 使用更合理的初始化范围\n",
    "                self.W1 = tf.Variable(shape=[28*28, 128], initial_value=tf.random.uniform(shape=[28*28, 128], minval=-0.05, maxval=0.05))\n",
    "                self.b1 = tf.Variable(shape=[128], initial_value=tf.zeros(128))\n",
    "                self.W2 = tf.Variable(shape=[128, 10], initial_value=tf.random.uniform(shape=[128, 10], minval=-0.05, maxval=0.05))\n",
    "                self.b2 = tf.Variable(shape=[10], initial_value=tf.zeros(10))\n",
    "                ####################\n",
    "        def __call__(self, x):\n",
    "                ####################\n",
    "                '''实现模型函数体，返回未归一化的logits'''\n",
    "                flat_x = tf.reshape(x, shape=[-1, 28*28])  # to match matrix multiply\n",
    "                h = tf.tanh(tf.matmul(flat_x, self.W1) + self.b1)  # 使用tanh激活函数\n",
    "                logits = tf.matmul(h, self.W2) + self.b2\n",
    "                ####################\n",
    "                return logits\n",
    "\n",
    "# 实例化模型\n",
    "model = myModel()\n",
    "\n",
    "# 调整优化器的学习率\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:44:57.238065Z",
     "start_time": "2025-03-19T07:44:57.231976Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:45:23.068606Z",
     "start_time": "2025-03-19T07:45:08.090232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 2.3176684 ; accuracy 0.07746667\n",
      "epoch 1 : loss 2.3156357 ; accuracy 0.08005\n",
      "epoch 2 : loss 2.313608 ; accuracy 0.0826\n",
      "epoch 3 : loss 2.3115847 ; accuracy 0.084866665\n",
      "epoch 4 : loss 2.3095663 ; accuracy 0.087633334\n",
      "epoch 5 : loss 2.3075516 ; accuracy 0.09026667\n",
      "epoch 6 : loss 2.3055418 ; accuracy 0.09301667\n",
      "epoch 7 : loss 2.303536 ; accuracy 0.096533336\n",
      "epoch 8 : loss 2.3015337 ; accuracy 0.09978333\n",
      "epoch 9 : loss 2.2995355 ; accuracy 0.102916665\n",
      "epoch 10 : loss 2.2975407 ; accuracy 0.10653333\n",
      "epoch 11 : loss 2.295549 ; accuracy 0.11025\n",
      "epoch 12 : loss 2.293561 ; accuracy 0.11385\n",
      "epoch 13 : loss 2.2915761 ; accuracy 0.11835\n",
      "epoch 14 : loss 2.2895942 ; accuracy 0.1222\n",
      "epoch 15 : loss 2.287615 ; accuracy 0.12633333\n",
      "epoch 16 : loss 2.2856386 ; accuracy 0.13056667\n",
      "epoch 17 : loss 2.283665 ; accuracy 0.13583334\n",
      "epoch 18 : loss 2.2816935 ; accuracy 0.14128333\n",
      "epoch 19 : loss 2.2797241 ; accuracy 0.14635\n",
      "epoch 20 : loss 2.2777574 ; accuracy 0.15235\n",
      "epoch 21 : loss 2.275792 ; accuracy 0.15853333\n",
      "epoch 22 : loss 2.273829 ; accuracy 0.1648\n",
      "epoch 23 : loss 2.2718673 ; accuracy 0.17163333\n",
      "epoch 24 : loss 2.2699075 ; accuracy 0.17855\n",
      "epoch 25 : loss 2.2679489 ; accuracy 0.18571667\n",
      "epoch 26 : loss 2.2659917 ; accuracy 0.1932\n",
      "epoch 27 : loss 2.264036 ; accuracy 0.20071666\n",
      "epoch 28 : loss 2.2620807 ; accuracy 0.20788333\n",
      "epoch 29 : loss 2.260127 ; accuracy 0.2157\n",
      "epoch 30 : loss 2.2581737 ; accuracy 0.22425\n",
      "epoch 31 : loss 2.2562208 ; accuracy 0.2321\n",
      "epoch 32 : loss 2.2542694 ; accuracy 0.24013333\n",
      "epoch 33 : loss 2.2523177 ; accuracy 0.24851666\n",
      "epoch 34 : loss 2.2503664 ; accuracy 0.25668332\n",
      "epoch 35 : loss 2.2484157 ; accuracy 0.26486668\n",
      "epoch 36 : loss 2.2464645 ; accuracy 0.27281666\n",
      "epoch 37 : loss 2.2445135 ; accuracy 0.28075\n",
      "epoch 38 : loss 2.2425623 ; accuracy 0.28798333\n",
      "epoch 39 : loss 2.2406104 ; accuracy 0.2956\n",
      "epoch 40 : loss 2.238659 ; accuracy 0.30261666\n",
      "epoch 41 : loss 2.2367063 ; accuracy 0.31003332\n",
      "epoch 42 : loss 2.2347531 ; accuracy 0.31681666\n",
      "epoch 43 : loss 2.2327993 ; accuracy 0.3241\n",
      "epoch 44 : loss 2.2308447 ; accuracy 0.33096668\n",
      "epoch 45 : loss 2.228889 ; accuracy 0.33775\n",
      "epoch 46 : loss 2.2269323 ; accuracy 0.3441\n",
      "epoch 47 : loss 2.2249744 ; accuracy 0.35128334\n",
      "epoch 48 : loss 2.223015 ; accuracy 0.35778335\n",
      "epoch 49 : loss 2.2210548 ; accuracy 0.36491665\n",
      "test loss 2.2186668 ; accuracy 0.3733\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(50):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取训练数据和标签\n",
    "train_data, train_labels = mnist_dataset()[0]\n",
    "\n",
    "# 绘制前 25 个数字\n",
    "num_images = 25\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axes = axes.ravel()  # 将 5x5 的 axes 数组展平为 1x25\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[i].imshow(train_data[i], cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {train_labels[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-19T07:45:25.204963Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设我们在训练过程中记录了每个 epoch 的损失和准确率\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# 模拟训练过程中的数据（可以替换为实际训练时记录的数据）\n",
    "for epoch in range(50):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracies.append(accuracy.numpy())\n",
    "\n",
    "    test_loss, test_accuracy = test(model, \n",
    "                                    tf.constant(test_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(test_data[1], dtype=tf.int64))\n",
    "    test_losses.append(test_loss.numpy())\n",
    "    test_accuracies.append(test_accuracy.numpy())\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(50), train_losses, label='Train Loss')\n",
    "plt.plot(range(50), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制准确率曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(50), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(50), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 重置模型\n",
    "model = myModel()\n",
    "\n",
    "# 保存训练过程中的梯度信息\n",
    "all_gradients = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# 再训练一遍，这次保存梯度\n",
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(30):  # 训练30轮\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(tf.constant(train_data[0][:1000], dtype=tf.float32))  # 使用部分数据加速训练\n",
    "        loss = compute_loss(logits, tf.constant(train_data[1][:1000], dtype=tf.int64))\n",
    "    \n",
    "    # 计算梯度\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    \n",
    "    # 保存梯度的范数\n",
    "    grad_norms = [tf.norm(g).numpy() for g in grads]\n",
    "    all_gradients.append(grad_norms)\n",
    "    \n",
    "    # 应用梯度\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01 * g)\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = compute_accuracy(logits, tf.constant(train_data[1][:1000], dtype=tf.int64))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    accuracies.append(accuracy.numpy())\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.numpy():.4f}, Accuracy = {accuracy.numpy():.4f}\")\n",
    "\n",
    "# 可视化梯度变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, name in enumerate([\"W1\", \"W2\", \"b1\", \"b2\"]):\n",
    "    plt.plot(range(len(all_gradients)), [g[i] for g in all_gradients], label=f\"grad_{name}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Gradient Norm\")\n",
    "plt.title(\"Gradient Change During Training\")\n",
    "plt.legend()\n",
    "\n",
    "# 可视化损失和准确率\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(losses)), losses, label=\"Loss\")\n",
    "plt.plot(range(len(accuracies)), accuracies, label=\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 可视化模型学习到的特征（第一层权重）\n",
    "plt.figure(figsize=(12, 12))\n",
    "W1_vis = model.W1.numpy().T  # 转置权重矩阵以便可视化\n",
    "n_neurons = min(100, W1_vis.shape[0])  # 可视化最多100个神经元\n",
    "\n",
    "grid_size = int(np.ceil(np.sqrt(n_neurons)))\n",
    "for i in range(n_neurons):\n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    # 重塑权重为28x28的图像形式\n",
    "    plt.imshow(W1_vis[i].reshape(28, 28), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"First Layer Weights (Features Learned)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 比较模型预测和实际标签\n",
    "sample_indices = np.random.choice(len(test_data[0]), 10)\n",
    "samples = tf.constant(test_data[0][sample_indices], dtype=tf.float32)\n",
    "true_labels = test_data[1][sample_indices]\n",
    "\n",
    "# 获取预测\n",
    "logits = model(samples)\n",
    "probabilities = tf.nn.softmax(logits).numpy()\n",
    "predictions = tf.argmax(logits, axis=1).numpy()\n",
    "\n",
    "# 可视化样本、预测概率和真实标签\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(10):\n",
    "    # 显示原始图像\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(test_data[0][sample_indices[i]], cmap='gray')\n",
    "    plt.title(f\"True: {true_labels[i]}, Pred: {predictions[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 显示每个样本的预测概率分布\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.bar(range(10), probabilities[i])\n",
    "    plt.title(f\"Sample {i+1}\")\n",
    "    plt.xlabel(\"Digit Class\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.axvline(x=true_labels[i], color='r', linestyle='--', label='True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3",
   "language": "python",
   "name": "python3.6.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
